\input{src/header}											% bindet Header ein (WICHTIG)
\usepackage{graphicx}
\usepackage{fancyvrb}

\newcommand{\dozent}{Prof. Dr. Agn`es Voisard, Nicolas Lehmann}					% <-- Names des Dozenten eintragen
\newcommand{\tutor}{Nicolas Lehmann}						% <-- Name eurer Tutoriun eintragen
\newcommand{\tutoriumNo}{10}				% <-- Nummer im KVV nachschauen
\newcommand{\projectNo}{5}									% <-- Nummer des Übungszettels
\newcommand{\veranstaltung}{Datenbanksysteme}	% <-- Name der Lehrveranstaltung eintragen
\newcommand{\semester}{SoSe 2017}						% <-- z.B. SoSe 17, WiSe 17/18
\newcommand{\studenten}{Boyan Hristov, Julian Habib}			% <-- Hier eure Namen eintragen
% /////////////////////// BEGIN DOKUMENT /////////////////////////

\begin{document}

\input{src/titlepage}										% erstellt die Titelseite

Link zum Git Repository: \url{https://github.com/BoyanH/Freie-Universitaet-Berlin/tree/master/Datenbanksysteme/Solutions/homework5}

% /////////////////////// Aufgabe 1 /////////////////////////

\section*{1. Aufgabe}

\begin{enumerate}

\item[a)]

\begin{align*}
\text{Minimal F} = \{ & \\
& \{B\} \rightarrow \{C\} \\
& \{B\} \rightarrow \{D\} \\
& \{C\} \rightarrow \{E\} \\
& \{D\} \rightarrow \{E\} \\
& \{AE\} \rightarrow \{D\} \\
\} & \\ \\
& \Rightarrow \\
F+ = \{ & \\
& \{B\} \rightarrow \{C\} \\
& \{B\} \rightarrow \{D\} \\
& \{B\} \rightarrow \{CD\} \\
& \{C\} \rightarrow \{E\} \\
& \{D\} \rightarrow \{E\} \\
& \{AE\} \rightarrow \{D\} \\
& \{B\} \rightarrow \{C\} \\
& \{B\} \rightarrow \{D\} \\
& \{BC\} \rightarrow \{CDE\} \\
& \{BD\} \rightarrow \{CDE\} \\
& \{CD\} \rightarrow \{E\} \\
& \{AB\} \rightarrow \{CDE\} \\
& ... \\
\} &
\end{align*}

Damit ist AB ein Key / Schlüssel.

\item[b)]
Pseudotransitivität wäre hier
\begin{align*}
& \{C\} \rightarrow \{E\} \land \{AE\} \rightarrow D \\
\Rightarrow & \{AC\} \rightarrow \{D\}
\end{align*}

\item[c)]
\begin{align*}
& \{BE\} \rightarrow \{B\} \text{(Reflexivity)} \\
\Rightarrow & \{A\} \rightarrow \{BE\} \land \{BE\} \rightarrow \{B\} \\
\Rightarrow & \{A\} \rightarrow \{B\} (Transitivity) \\
\Rightarrow & \{A\} \rightarrow \{B\} \land \{A\} \rightarrow \{A\} \text{(Reflexivity)} \\
\Rightarrow & \{A\} \rightarrow \{AB\} \text{(Union)} \land \{AB\} \rightarrow \{C\} \\
\Rightarrow & \{A\} \rightarrow \{C\} \text{(Transitivity)} \\
\Rightarrow & \{A\} \rightarrow \{C\} \land \{C\} \rightarrow \{D\} \\
\Rightarrow & \{A\} \rightarrow \{D\} \text{(Transitivity)}
\end{align*}

\end{enumerate}

\section*{2. Aufgabe}

\begin{enumerate}

\item[a)]

Attributhülle von \{A\}

Es gibt keine funktionale Abhängigkeiten für den Attribut A, deswegen ist seine Attributhülle \{A\} den Attribut selbst. 

Attributhülle von \{B\}

\begin{align*}
& \{B\} \rightarrow \{AD\} \Rightarrow \{B\} \rightarrow \{D\} \text{(Reflexivity)} \\
\Rightarrow & \{B\} \rightarrow \{D\} \land \{D\} \rightarrow \{BC\} \\
\Rightarrow & \{B\} \rightarrow \{C\} \text{(Transitivity)} \\ \\ \\
%
\Rightarrow & \{B\} \rightarrow \{C\} \land \{B\} \rightarrow \{D\} \\
\Rightarrow & \{B\} \rightarrow \{CD\} \text{(Union)} \\
\Rightarrow & \{B\} \rightarrow \{CD\} \land \{CD\} \{AEF\} \\
\Rightarrow & \{B\} \rightarrow \{AEF\} \text{(Transitivity)} \\ \\ \\
%
& \{B\} \rightarrow \{B\} \text{(Reflexivity)} \\ \\ \\
%
& \{B\} \rightarrow \{ABCDEF\}
\end{align*}
Attributhülle von \{B\} = $B^{+}$ = \{ABCDEF\}. Deswegen ist B ein Schlüssel. 

\item[b)]
Wie wir bei Aufgabenteil a) gesehen haben ist B ein Schlüsel.
Wegen $\{D\} \rightarrow \{BC\}$ ist D ein weiteres Schlüssel. Wegen $\{F\} \rightarrow \{ABD\}$ ist F auch ein Schlüssel. Von $\{E\} \rightarrow \{CD\}$ ist E auch Schlüssel. Weitere Kandidaten gibt es nicht, da per Definition ein Schlüssel das kleinste Superkey ist und alle andere Superkeys haben mindestens 2 Attributen.

Kandidaten für Schlüssel: B, D, E, F

\item[c)]

Wir haben aus allen Kandidaten B für Schlüssel gewählt, da dieser am trivialsten zu sehen ist und auch alphabetisch der ersten Attribut ist. Da es keine feste Regeln gibt, welcher von den Kandidaten zu einem Schlüssel wird, war unsere Entscheidung nicht so schwer zu treffen.

\end{enumerate}

\section*{3. Aufgabe}

\begin{align*}
FD(R_2) = \{ & \\
& \{A\} \rightarrow \{D, E\} \\
& \{B\} \rightarrow \{C, D\} \\
& \{C\} \rightarrow \{A\} \\
& \{D, E\} \rightarrow \{A, C\} \\
\} & \\ \\
%
& \{B\} \rightarrow \{CD\} \Rightarrow \{B\} \rightarrow \{C\} \text{(Reflexivity)} \\
\Rightarrow & \{B\} \rightarrow \{C\} \land \{C\} \rightarrow \{A\} \\
\Rightarrow & \{B\} \rightarrow \{A\} \text{(Transitivity)} \\
\Rightarrow & \{B\} \rightarrow \{A\} \land \{A\} \rightarrow \{DE\} \\
\Rightarrow & \{B\} \rightarrow \{DE\} \text{(Transitivity)} \\
\Rightarrow & \{B\} \rightarrow \{D\} \text{(Reflexivity)} \\
\Rightarrow & \text{Dependency } \{B\} \rightarrow \{CD\} \text{ kann ducrh } \{B\} \rightarrow \{C\} \text{ substituiert werden} \\ \\
%
& \{C\} \rightarrow \{A\} \Rightarrow \text{ Dependency } \{DE\} \rightarrow \{A\} \text{ ist wegen dependency} \{DE\} \rightarrow \{C\} \text{ nicht notwendig} \\ \\
%
\Leftrightarrow \\
FD(R_2) = \{ & \\
& \{A\} \rightarrow \{D, E\} \\
& \{B\} \rightarrow \{C\} \\
& \{C\} \rightarrow \{A\} \\
& \{D, E\} \rightarrow \{C\} \\
\} & \\ \\ \\
\Leftrightarrow & \text{(Decomposition)} \\ \\
FD(R_2) = \{ & \\
& \{A\} \rightarrow \{D\} \\
& \{A\} \rightarrow \{E\} \\
& \{B\} \rightarrow \{C\} \\
& \{C\} \rightarrow \{A\} \\
& \{D, E\} \rightarrow \{C\} \\
\} & \\ \\ \\
\end{align*}

Damit sind alle Anforderungen aus der Vorlesung erfüllt $\rightarrow$ 

1. Every right side of a dependency in F is a single attribute.
(apply decomposition) \\
2. For no $ X \rightarrow A $ in F is the set $F - \{ X \rightarrow A \}$
equivalent to F. \\
3. For no $ X \rightarrow A $ in F and subset Z of X is
$F - \{ X \rightarrow A \} \cup \{ Z \rightarrow A \}$ equivalent to F.

\section*{Aufgabe 5}

\begin{enumerate}

\item[a)]
Der Scraper liest mehrere Seiten von einem Web Application, und zwar auf mehrere Sprachen über die Jahren 2000 bis 2015 inklusive.
Von den Seiten nimmt er immer die erste Tabelle, davon alle Zeilen mit rechtsbündigen Text. Für jede Zeile in der Tabelle speichert 
der Scraper eine Zeile in .csv Datei, wo Land, Jahr und die Zellen in der Tabelle nacheinander stehen. \\ \\

Leider könnten wir den Scraper nicht ausführen, da diese Seiten nicht erreichbar waren. Laut Julian aber speichtert der Scraper
alle Informationen von Hunderassen der Jahre 2000-2016 in verschiedene Sprachen in einer CSV Datei. \\ \\

Nach mehrere versuche und VPN (war es überhaupt notwendig?) und nachdem es spät genug war, damit nicht alle wild die Seite scrapen, 
haben wir gesehen, dass auf der Seite eine Tabelle von den besten Hunden in Wettbewerben steht, je mit Name, Geschlecht, abgeschlossene Wettbewerbe usw.
Jede Zeile aus alle solchen Tabellen wird eine Zeile in unsere CSV Datei und dazu werden noch zwei Spalten für Land und Jahr eingefügt. Im großen und ganzen
wird eine perfekte CSV Datei für das DBS Projekt im nächsten Semester :)

\item[b)]

\begin{lstlisting}[style=py]
from bs4 import BeautifulSoup
import requests
import re
from collections import Counter

PER_PAGE_NAVIGATION_CLASS = 'seitenweise_navigation'
PAGINATION_CLASS = 'pagination'
MAIN_PAGE_URL = 'https://www.heise.de/thema/https'
SITE_SUFFIX = '?seite='

# this function returns a soup page object
def getPage(url):
    r = requests.get(url)
    data = r.text
    spobj = BeautifulSoup(data, "lxml")
    return spobj

# pushes article soup objects into the passed articles array
def parsePage(soup, articles):
    # Summary:
    # In each page, get the <aside class="recommendations"> elements
    # Inside, search for a <div class="recommendation">
    # Every div of that kind is an article, add it to the articles array passed to the function

    recommendationsAside = soup.find('aside', class_ = 'recommendations')
    recommendations = recommendationsAside.find_all('div', class_ = 'recommendation')

    for rec in recommendations:
        articles.append(rec)

# returns array
def getHeadings(articles):
    # Summary:
    # get all <header> tags in an article, 
    # remove new lines and extra spaces and return the result

    headings = []

    for article in articles:
        heading = article.find('header')
        headingText = heading.text.replace('\n', '').strip()
        headings.append(headingText)
    
    return headings

# returns Dictionary<Word, Count>
def getPerWordCountInHeadings(headings):
    # Summary:
    # All symbol are removed from the article except for the one in the german language and dashes
    # Words are then separated by dashes and spaces and counted

    perWordCount = {}

    for heading in headings:
        #re.sub...
        words = re.sub('[^a-zA-Z äÄöÖüÜß\-\'`]', '', heading).replace('-', ' ').lower().split(' ')
        
        for word in words:
            if len(word) < 3:
                continue
            if not word in perWordCount:
                perWordCount[word] = 1
            else:
                perWordCount[word] += 1

    return perWordCount

# returns array
def getTopNWords(wordCountDict, n):
    # Summary:
    # Using the functionality from collections.Counter get the top 3 keys 
    # in the dictionary sorted by their value and push them to the returned array

    counter = Counter(wordCountDict)
    topNWords = []
    
    for word in counter.most_common(n):
        topNWords.append(word)

    return topNWords


# scraper website: greyhound-data.com
def main():

    articles = []

    mainPage = getPage(MAIN_PAGE_URL)
    navigation = mainPage.find_all('nav', class_ = PER_PAGE_NAVIGATION_CLASS)[0]
    navItemsContainer = mainPage.find('span', class_ = PAGINATION_CLASS)
    nonActivePages = navItemsContainer.find_all('a')
    furtherPages = len(nonActivePages)

    parsePage(mainPage, articles);

    for page in range(1, furtherPages+1):
        furtherPage = getPage(MAIN_PAGE_URL + SITE_SUFFIX + str(page))
        parsePage(furtherPage, articles)

    headings = getHeadings(articles)
    perWordCount = getPerWordCountInHeadings(headings)
    top3Words = getTopNWords(perWordCount, 3)

    print("\nDie meist benutzten 3 Wörter mit mehr als 3 Buchstaben inklusive sind: \n")
    for word in top3Words:
        print("{0}: {1} mal".format(word[0], word[1]))
    print("\n\nWichtig: Nur die Artikeln link die zu HTTPS releavnt sind wurden analyziert!\n")


# main program

if __name__ == '__main__':
    main()
\end{lstlisting} 


\end{enumerate}



% /////////////////////// END DOKUMENT /////////////////////////
\end{document}
